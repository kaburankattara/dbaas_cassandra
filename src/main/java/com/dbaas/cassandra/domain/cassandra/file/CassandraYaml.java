package com.dbaas.cassandra.domain.cassandra.file;

import java.io.File;

import com.dbaas.cassandra.domain.serverManager.instance.Instance;
import com.dbaas.cassandra.domain.user.LoginUser;

public class CassandraYaml {
	
	private String fileName;
	
	private String detail;

	/**
	 * 設定ファイルマネージャーを生成
	 * @return
	 */
	public static CassandraYaml createManager(LoginUser user) {
		return new CassandraYaml(user);
	}
	
	public CassandraYaml(LoginUser user) {
		// TODO 定数化
		this.fileName = "/etc/cassandra/conf/cassandra.yaml";
		//this.fileName = PATH_CASSANDRA_HOME + PATH_CONF + FILE_CASSANDRA_YAML;
	}

	public void create(LoginUser user, Instance instance) {
		// ファイル内容を生成
		createBuffer(user, instance);
	}
	
    public void delete() {
        File file = new File(fileName);
 
        //deleteメソッドを使用してファイルを削除する
        file.delete();
    }
	
    public String getDetail() {
       return detail;
    }
	
    public String getFileName() {
       return fileName;
    }
	
	private void createBuffer(LoginUser user, Instance instance) {
		StringBuilder sb = new StringBuilder();
		sb.append("# Cassandra storage config YAML\n");
		sb.append("\n");
		sb.append("# NOTE:\n");
		sb.append("#   See http://wiki.apache.org/cassandra/StorageConfiguration for\n");
		sb.append("#   full explanations of configuration directives\n");
		sb.append("# /NOTE\n");
		sb.append("\n");
		sb.append("# The name of the cluster. This is mainly used to prevent machines in\n");
		sb.append("# one logical cluster from joining another.\n");
		// クラスタ名にはユーザIDを指定する
		sb.append("cluster_name: 'test Cluster'\n");
		sb.append("\n");
		sb.append("# This defines the number of tokens randomly assigned to this node on the ring\n");
		sb.append("# The more tokens, relative to other nodes, the larger the proportion of data\n");
		sb.append("# that this node will store. You probably want all nodes to have the same number\n");
		sb.append("# of tokens assuming they have equal hardware capability.\n");
		sb.append("#\n");
		sb.append("# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n");
		sb.append("# and will use the initial_token as described below.\n");
		sb.append("#\n");
		sb.append("# Specifying initial_token will override this setting on the node's initial start,\n");
		sb.append("# on subsequent starts, this setting will apply even if initial token is set.\n");
		sb.append("#\n");
		sb.append("# If you already have a cluster with 1 token per node, and wish to migrate to \n");
		sb.append("# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\n");
		sb.append("num_tokens: 256\n");
		sb.append("\n");
		sb.append("# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n");
		sb.append("# algorithm attempts to choose tokens in a way that optimizes replicated load over\n");
		sb.append("# the nodes in the datacenter for the replication strategy used by the specified\n");
		sb.append("# keyspace.\n");
		sb.append("#\n");
		sb.append("# The load assigned to each node will be close to proportional to its number of\n");
		sb.append("# vnodes.\n");
		sb.append("#\n");
		sb.append("# Only supported with the Murmur3Partitioner.\n");
		sb.append("# allocate_tokens_for_keyspace: KEYSPACE\n");
		sb.append("\n");
		sb.append("# initial_token allows you to specify tokens manually.  While you can use it with\n");
		sb.append("# vnodes (num_tokens > 1, above) -- in which case you should provide a \n");
		sb.append("# comma-separated list -- it's primarily used when adding nodes to legacy clusters \n");
		sb.append("# that do not have vnodes enabled.\n");
		sb.append("# initial_token:\n");
		sb.append("\n");
		sb.append("# See http://wiki.apache.org/cassandra/HintedHandoff\n");
		sb.append("# May either be \\\"true\\\" or \\\"false\\\" to enable globally\n");
		sb.append("hinted_handoff_enabled: true\n");
		sb.append("\n");
		sb.append("# When hinted_handoff_enabled is true, a black list of data centers that will not\n");
		sb.append("# perform hinted handoff\n");
		sb.append("# hinted_handoff_disabled_datacenters:\n");
		sb.append("#    - DC1\n");
		sb.append("#    - DC2\n");
		sb.append("\n");
		sb.append("# this defines the maximum amount of time a dead host will have hints\n");
		sb.append("# generated.  After it has been dead this long, new hints for it will not be\n");
		sb.append("# created until it has been seen alive and gone down again.\n");
		sb.append("max_hint_window_in_ms: 10800000 # 3 hours\n");
		sb.append("\n");
		sb.append("# Maximum throttle in KBs per second, per delivery thread.  This will be\n");
		sb.append("# reduced proportionally to the number of nodes in the cluster.  (If there\n");
		sb.append("# are two nodes in the cluster, each delivery thread will use the maximum\n");
		sb.append("# rate; if there are three, each will throttle to half of the maximum,\n");
		sb.append("# since we expect two nodes to be delivering hints simultaneously.)\n");
		sb.append("hinted_handoff_throttle_in_kb: 1024\n");
		sb.append("\n");
		sb.append("# Number of threads with which to deliver hints;\n");
		sb.append("# Consider increasing this number when you have multi-dc deployments, since\n");
		sb.append("# cross-dc handoff tends to be slower\n");
		sb.append("max_hints_delivery_threads: 2\n");
		sb.append("\n");
		sb.append("# Directory where Cassandra should store hints.\n");
		sb.append("# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n");
		sb.append("# hints_directory: /var/lib/cassandra/hints\n");
		sb.append("\n");
		sb.append("# How often hints should be flushed from the internal buffers to disk.\n");
		sb.append("# Will *not* trigger fsync.\n");
		sb.append("hints_flush_period_in_ms: 10000\n");
		sb.append("\n");
		sb.append("# Maximum size for a single hints file, in megabytes.\n");
		sb.append("max_hints_file_size_in_mb: 128\n");
		sb.append("\n");
		sb.append("# Compression to apply to the hint files. If omitted, hints files\n");
		sb.append("# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n");
		sb.append("# are supported.\n");
		sb.append("#hints_compression:\n");
		sb.append("#   - class_name: LZ4Compressor\n");
		sb.append("#     parameters:\n");
		sb.append("#         -\n");
		sb.append("\n");
		sb.append("# Maximum throttle in KBs per second, total. This will be\n");
		sb.append("# reduced proportionally to the number of nodes in the cluster.\n");
		sb.append("batchlog_replay_throttle_in_kb: 1024\n");
		sb.append("\n");
		sb.append("# Authentication backend, implementing IAuthenticator; used to identify users\n");
		sb.append("# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n");
		sb.append("# PasswordAuthenticator}.\n");
		sb.append("#\n");
		sb.append("# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n");
		sb.append("# - PasswordAuthenticator relies on username/password pairs to authenticate\n");
		sb.append("#   users. It keeps usernames and hashed passwords in system_auth.roles table.\n");
		sb.append("#   Please increase system_auth keyspace replication factor if you use this authenticator.\n");
		sb.append("#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\n");
		sb.append("authenticator: AllowAllAuthenticator\n");
		sb.append("\n");
		sb.append("# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n");
		sb.append("# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n");
		sb.append("# CassandraAuthorizer}.\n");
		sb.append("#\n");
		sb.append("# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n");
		sb.append("# - CassandraAuthorizer stores permissions in system_auth.role_permissions table. Please\n");
		sb.append("#   increase system_auth keyspace replication factor if you use this authorizer.\n");
		sb.append("authorizer: AllowAllAuthorizer\n");
		sb.append("\n");
		sb.append("# Part of the Authentication & Authorization backend, implementing IRoleManager; used\n");
		sb.append("# to maintain grants and memberships between roles.\n");
		sb.append("# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n");
		sb.append("# which stores role information in the system_auth keyspace. Most functions of the\n");
		sb.append("# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n");
		sb.append("# actually implements authentication, most of this functionality will be unavailable.\n");
		sb.append("#\n");
		sb.append("# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n");
		sb.append("#   increase system_auth keyspace replication factor if you use this role manager.\n");
		sb.append("role_manager: CassandraRoleManager\n");
		sb.append("\n");
		sb.append("# Validity period for roles cache (fetching granted roles can be an expensive\n");
		sb.append("# operation depending on the role manager, CassandraRoleManager is one example)\n");
		sb.append("# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n");
		sb.append("# after the period specified here, become eligible for (async) reload.\n");
		sb.append("# Defaults to 2000, set to 0 to disable caching entirely.\n");
		sb.append("# Will be disabled automatically for AllowAllAuthenticator.\n");
		sb.append("roles_validity_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# Refresh interval for roles cache (if enabled).\n");
		sb.append("# After this interval, cache entries become eligible for refresh. Upon next\n");
		sb.append("# access, an async reload is scheduled and the old value returned until it\n");
		sb.append("# completes. If roles_validity_in_ms is non-zero, then this must be\n");
		sb.append("# also.\n");
		sb.append("# Defaults to the same value as roles_validity_in_ms.\n");
		sb.append("# roles_update_interval_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# Validity period for permissions cache (fetching permissions can be an\n");
		sb.append("# expensive operation depending on the authorizer, CassandraAuthorizer is\n");
		sb.append("# one example). Defaults to 2000, set to 0 to disable.\n");
		sb.append("# Will be disabled automatically for AllowAllAuthorizer.\n");
		sb.append("permissions_validity_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# Refresh interval for permissions cache (if enabled).\n");
		sb.append("# After this interval, cache entries become eligible for refresh. Upon next\n");
		sb.append("# access, an async reload is scheduled and the old value returned until it\n");
		sb.append("# completes. If permissions_validity_in_ms is non-zero, then this must be\n");
		sb.append("# also.\n");
		sb.append("# Defaults to the same value as permissions_validity_in_ms.\n");
		sb.append("# permissions_update_interval_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# Validity period for credentials cache. This cache is tightly coupled to\n");
		sb.append("# the provided PasswordAuthenticator implementation of IAuthenticator. If\n");
		sb.append("# another IAuthenticator implementation is configured, this cache will not\n");
		sb.append("# be automatically used and so the following settings will have no effect.\n");
		sb.append("# Please note, credentials are cached in their encrypted form, so while\n");
		sb.append("# activating this cache may reduce the number of queries made to the\n");
		sb.append("# underlying table, it may not  bring a significant reduction in the\n");
		sb.append("# latency of individual authentication attempts.\n");
		sb.append("# Defaults to 2000, set to 0 to disable credentials caching.\n");
		sb.append("credentials_validity_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# Refresh interval for credentials cache (if enabled).\n");
		sb.append("# After this interval, cache entries become eligible for refresh. Upon next\n");
		sb.append("# access, an async reload is scheduled and the old value returned until it\n");
		sb.append("# completes. If credentials_validity_in_ms is non-zero, then this must be\n");
		sb.append("# also.\n");
		sb.append("# Defaults to the same value as credentials_validity_in_ms.\n");
		sb.append("# credentials_update_interval_in_ms: 2000\n");
		sb.append("\n");
		sb.append("# The partitioner is responsible for distributing groups of rows (by\n");
		sb.append("# partition key) across nodes in the cluster.  You should leave this\n");
		sb.append("# alone for new clusters.  The partitioner can NOT be changed without\n");
		sb.append("# reloading all data, so when upgrading you should set this to the\n");
		sb.append("# same partitioner you were already using.\n");
		sb.append("#\n");
		sb.append("# Besides Murmur3Partitioner, partitioners included for backwards\n");
		sb.append("# compatibility include RandomPartitioner, ByteOrderedPartitioner, and\n");
		sb.append("# OrderPreservingPartitioner.\n");
		sb.append("#\n");
		sb.append("partitioner: org.apache.cassandra.dht.Murmur3Partitioner\n");
		sb.append("\n");
		sb.append("# Directories where Cassandra should store data on disk.  Cassandra\n");
		sb.append("# will spread data evenly across them, subject to the granularity of\n");
		sb.append("# the configured compaction strategy.\n");
		sb.append("# If not set, the default directory is $CASSANDRA_HOME/data/data.\n");
		sb.append("# data_file_directories:\n");
		sb.append("#     - /var/lib/cassandra/data\n");
		sb.append("\n");
		sb.append("# commit log.  when running on magnetic HDD, this should be a\n");
		sb.append("# separate spindle than the data directories.\n");
		sb.append("# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\n");
		sb.append("# commitlog_directory: /var/lib/cassandra/commitlog\n");
		sb.append("\n");
		sb.append("# Enable / disable CDC functionality on a per-node basis. This modifies the logic used\n");
		sb.append("# for write path allocation rejection (standard: never reject. cdc: reject Mutation\n");
		sb.append("# containing a CDC-enabled table if at space limit in cdc_raw_directory).\n");
		sb.append("cdc_enabled: false\n");
		sb.append("\n");
		sb.append("# CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the\n");
		sb.append("# segment contains mutations for a CDC-enabled table. This should be placed on a\n");
		sb.append("# separate spindle than the data directories. If not set, the default directory is\n");
		sb.append("# $CASSANDRA_HOME/data/cdc_raw.\n");
		sb.append("# cdc_raw_directory: /var/lib/cassandra/cdc_raw\n");
		sb.append("\n");
		sb.append("# Policy for data disk failures:\n");
		sb.append("#\n");
		sb.append("# die\n");
		sb.append("#   shut down gossip and client transports and kill the JVM for any fs errors or\n");
		sb.append("#   single-sstable errors, so the node can be replaced.\n");
		sb.append("#\n");
		sb.append("# stop_paranoid\n");
		sb.append("#   shut down gossip and client transports even for single-sstable errors,\n");
		sb.append("#   kill the JVM for errors during startup.\n");
		sb.append("#\n");
		sb.append("# stop\n");
		sb.append("#   shut down gossip and client transports, leaving the node effectively dead, but\n");
		sb.append("#   can still be inspected via JMX, kill the JVM for errors during startup.\n");
		sb.append("#\n");
		sb.append("# best_effort\n");
		sb.append("#    stop using the failed disk and respond to requests based on\n");
		sb.append("#    remaining available sstables.  This means you WILL see obsolete\n");
		sb.append("#    data at CL.ONE!\n");
		sb.append("#\n");
		sb.append("# ignore\n");
		sb.append("#    ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\n");
		sb.append("disk_failure_policy: stop\n");
		sb.append("\n");
		sb.append("# Policy for commit disk failures:\n");
		sb.append("#\n");
		sb.append("# die\n");
		sb.append("#   shut down gossip and Thrift and kill the JVM, so the node can be replaced.\n");
		sb.append("#\n");
		sb.append("# stop\n");
		sb.append("#   shut down gossip and Thrift, leaving the node effectively dead, but\n");
		sb.append("#   can still be inspected via JMX.\n");
		sb.append("#\n");
		sb.append("# stop_commit\n");
		sb.append("#   shutdown the commit log, letting writes collect but\n");
		sb.append("#   continuing to service reads, as in pre-2.0.5 Cassandra\n");
		sb.append("#\n");
		sb.append("# ignore\n");
		sb.append("#   ignore fatal errors and let the batches fail\n");
		sb.append("commit_failure_policy: stop\n");
		sb.append("\n");
		sb.append("# Maximum size of the native protocol prepared statement cache\n");
		sb.append("#\n");
		sb.append("# Valid values are either \\\"auto\\\" (omitting the value) or a value greater 0.\n");
		sb.append("#\n");
		sb.append("# Note that specifying a too large value will result in long running GCs and possbily\n");
		sb.append("# out-of-memory errors. Keep the value at a small fraction of the heap.\n");
		sb.append("#\n");
		sb.append("# If you constantly see \\\"prepared statements discarded in the last minute because\n");
		sb.append("# cache limit reached\\\" messages, the first step is to investigate the root cause\n");
		sb.append("# of these messages and check whether prepared statements are used correctly -\n");
		sb.append("# i.e. use bind markers for variable parts.\n");
		sb.append("#\n");
		sb.append("# Do only change the default value, if you really have more prepared statements than\n");
		sb.append("# fit in the cache. In most cases it is not neccessary to change this value.\n");
		sb.append("# Constantly re-preparing statements is a performance penalty.\n");
		sb.append("#\n");
		sb.append("# Default value (\\\"auto\\\") is 1/256th of the heap or 10MB, whichever is greater\n");
		sb.append("prepared_statements_cache_size_mb:\n");
		sb.append("\n");
		sb.append("# Maximum size of the Thrift prepared statement cache\n");
		sb.append("#\n");
		sb.append("# If you do not use Thrift at all, it is safe to leave this value at \\\"auto\\\".\n");
		sb.append("#\n");
		sb.append("# See description of 'prepared_statements_cache_size_mb' above for more information.\n");
		sb.append("#\n");
		sb.append("# Default value (\\\"auto\\\") is 1/256th of the heap or 10MB, whichever is greater\n");
		sb.append("thrift_prepared_statements_cache_size_mb:\n");
		sb.append("\n");
		sb.append("# Maximum size of the key cache in memory.\n");
		sb.append("#\n");
		sb.append("# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n");
		sb.append("# minimum, sometimes more. The key cache is fairly tiny for the amount of\n");
		sb.append("# time it saves, so it's worthwhile to use it at large numbers.\n");
		sb.append("# The row cache saves even more time, but must contain the entire row,\n");
		sb.append("# so it is extremely space-intensive. It's best to only use the\n");
		sb.append("# row cache if you have hot rows or static rows.\n");
		sb.append("#\n");
		sb.append("# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n");
		sb.append("#\n");
		sb.append("# Default value is empty to make it \\\"auto\\\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\n");
		sb.append("key_cache_size_in_mb:\n");
		sb.append("\n");
		sb.append("# Duration in seconds after which Cassandra should\n");
		sb.append("# save the key cache. Caches are saved to saved_caches_directory as\n");
		sb.append("# specified in this configuration file.\n");
		sb.append("#\n");
		sb.append("# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n");
		sb.append("# terms of I/O for the key cache. Row cache saving is much more expensive and\n");
		sb.append("# has limited use.\n");
		sb.append("#\n");
		sb.append("# Default is 14400 or 4 hours.\n");
		sb.append("key_cache_save_period: 14400\n");
		sb.append("\n");
		sb.append("# Number of keys from the key cache to save\n");
		sb.append("# Disabled by default, meaning all keys are going to be saved\n");
		sb.append("# key_cache_keys_to_save: 100\n");
		sb.append("\n");
		sb.append("# Row cache implementation class name. Available implementations:\n");
		sb.append("#\n");
		sb.append("# org.apache.cassandra.cache.OHCProvider\n");
		sb.append("#   Fully off-heap row cache implementation (default).\n");
		sb.append("#\n");
		sb.append("# org.apache.cassandra.cache.SerializingCacheProvider\n");
		sb.append("#   This is the row cache implementation availabile\n");
		sb.append("#   in previous releases of Cassandra.\n");
		sb.append("# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n");
		sb.append("\n");
		sb.append("# Maximum size of the row cache in memory.\n");
		sb.append("# Please note that OHC cache implementation requires some additional off-heap memory to manage\n");
		sb.append("# the map structures and some in-flight memory during operations before/after cache entries can be\n");
		sb.append("# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n");
		sb.append("# Do not specify more memory that the system can afford in the worst usual situation and leave some\n");
		sb.append("# headroom for OS block level cache. Do never allow your system to swap.\n");
		sb.append("#\n");
		sb.append("# Default value is 0, to disable row caching.\n");
		sb.append("row_cache_size_in_mb: 0\n");
		sb.append("\n");
		sb.append("# Duration in seconds after which Cassandra should save the row cache.\n");
		sb.append("# Caches are saved to saved_caches_directory as specified in this configuration file.\n");
		sb.append("#\n");
		sb.append("# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n");
		sb.append("# terms of I/O for the key cache. Row cache saving is much more expensive and\n");
		sb.append("# has limited use.\n");
		sb.append("#\n");
		sb.append("# Default is 0 to disable saving the row cache.\n");
		sb.append("row_cache_save_period: 0\n");
		sb.append("\n");
		sb.append("# Number of keys from the row cache to save.\n");
		sb.append("# Specify 0 (which is the default), meaning all keys are going to be saved\n");
		sb.append("# row_cache_keys_to_save: 100\n");
		sb.append("\n");
		sb.append("# Maximum size of the counter cache in memory.\n");
		sb.append("#\n");
		sb.append("# Counter cache helps to reduce counter locks' contention for hot counter cells.\n");
		sb.append("# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n");
		sb.append("# write entirely. With RF > 1 a counter cache hit will still help to reduce the duration\n");
		sb.append("# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n");
		sb.append("# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n");
		sb.append("# in memory, not the whole counter, so it's relatively cheap.\n");
		sb.append("#\n");
		sb.append("# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n");
		sb.append("#\n");
		sb.append("# Default value is empty to make it \\\"auto\\\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n");
		sb.append("# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\n");
		sb.append("counter_cache_size_in_mb:\n");
		sb.append("\n");
		sb.append("# Duration in seconds after which Cassandra should\n");
		sb.append("# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n");
		sb.append("# specified in this configuration file.\n");
		sb.append("#\n");
		sb.append("# Default is 7200 or 2 hours.\n");
		sb.append("counter_cache_save_period: 7200\n");
		sb.append("\n");
		sb.append("# Number of keys from the counter cache to save\n");
		sb.append("# Disabled by default, meaning all keys are going to be saved\n");
		sb.append("# counter_cache_keys_to_save: 100\n");
		sb.append("\n");
		sb.append("# saved caches\n");
		sb.append("# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\n");
		sb.append("# saved_caches_directory: /var/lib/cassandra/saved_caches\n");
		sb.append("\n");
		sb.append("# commitlog_sync may be either \\\"periodic\\\" or \\\"batch.\\\" \n");
		sb.append("# \n");
		sb.append("# When in batch mode, Cassandra won't ack writes until the commit log\n");
		sb.append("# has been fsynced to disk.  It will wait\n");
		sb.append("# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n");
		sb.append("# This window should be kept short because the writer threads will\n");
		sb.append("# be unable to do extra work while waiting.  (You may need to increase\n");
		sb.append("# concurrent_writes for the same reason.)\n");
		sb.append("#\n");
		sb.append("# commitlog_sync: batch\n");
		sb.append("# commitlog_sync_batch_window_in_ms: 2\n");
		sb.append("#\n");
		sb.append("# the other option is \\\"periodic\\\" where writes may be acked immediately\n");
		sb.append("# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n");
		sb.append("# milliseconds.\n");
		sb.append("commitlog_sync: periodic\n");
		sb.append("commitlog_sync_period_in_ms: 10000\n");
		sb.append("\n");
		sb.append("# The size of the individual commitlog file segments.  A commitlog\n");
		sb.append("# segment may be archived, deleted, or recycled once all the data\n");
		sb.append("# in it (potentially from each columnfamily in the system) has been\n");
		sb.append("# flushed to sstables.\n");
		sb.append("#\n");
		sb.append("# The default size is 32, which is almost always fine, but if you are\n");
		sb.append("# archiving commitlog segments (see commitlog_archiving.properties),\n");
		sb.append("# then you probably want a finer granularity of archiving; 8 or 16 MB\n");
		sb.append("# is reasonable.\n");
		sb.append("# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n");
		sb.append("# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n");
		sb.append("# This should be positive and less than 2048.\n");
		sb.append("#\n");
		sb.append("# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n");
		sb.append("# be set to at least twice the size of max_mutation_size_in_kb / 1024\n");
		sb.append("#\n");
		sb.append("commitlog_segment_size_in_mb: 32\n");
		sb.append("\n");
		sb.append("# Compression to apply to the commit log. If omitted, the commit log\n");
		sb.append("# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n");
		sb.append("# are supported.\n");
		sb.append("# commitlog_compression:\n");
		sb.append("#   - class_name: LZ4Compressor\n");
		sb.append("#     parameters:\n");
		sb.append("#         -\n");
		sb.append("\n");
		sb.append("# any class that implements the SeedProvider interface and has a\n");
		sb.append("# constructor that takes a Map<String, String> of parameters will do.\n");
		sb.append("seed_provider:\n");
		sb.append("    # Addresses of hosts that are deemed contact points. \n");
		sb.append("    # Cassandra nodes use this list of hosts to find each other and learn\n");
		sb.append("    # the topology of the ring.  You must change this if you are running\n");
		sb.append("    # multiple nodes!\n");
		sb.append("    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n");
		sb.append("      parameters:\n");
		sb.append("          # seeds is actually a comma-delimited list of addresses.\n");
		sb.append("          # Ex: \\\"<ip1>,<ip2>,<ip3>\\\"\n");
		// シードノードに自身のプライベートIPアドレスを追加する
		// TODO マルチノード起動を入れるときに変更が必要
		sb.append("          - seeds: \\\"127.0.0.1\\\"\n");
		sb.append("\n");
		sb.append("# For workloads with more data than can fit in memory, Cassandra's\n");
		sb.append("# bottleneck will be reads that need to fetch data from\n");
		sb.append("# disk. \\\"concurrent_reads\\\" should be set to (16 * number_of_drives) in\n");
		sb.append("# order to allow the operations to enqueue low enough in the stack\n");
		sb.append("# that the OS and drives can reorder them. Same applies to\n");
		sb.append("# \\\"concurrent_counter_writes\\\", since counter writes read the current\n");
		sb.append("# values before incrementing and writing them back.\n");
		sb.append("#\n");
		sb.append("# On the other hand, since writes are almost never IO bound, the ideal\n");
		sb.append("# number of \\\"concurrent_writes\\\" is dependent on the number of cores in\n");
		sb.append("# your system; (8 * number_of_cores) is a good rule of thumb.\n");
		sb.append("concurrent_reads: 32\n");
		sb.append("concurrent_writes: 32\n");
		sb.append("concurrent_counter_writes: 32\n");
		sb.append("\n");
		sb.append("# For materialized view writes, as there is a read involved, so this should\n");
		sb.append("# be limited by the less of concurrent reads or concurrent writes.\n");
		sb.append("concurrent_materialized_view_writes: 32\n");
		sb.append("\n");
		sb.append("# Maximum memory to use for sstable chunk cache and buffer pooling.\n");
		sb.append("# 32MB of this are reserved for pooling buffers, the rest is used as an\n");
		sb.append("# cache that holds uncompressed sstable chunks.\n");
		sb.append("# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n");
		sb.append("# so is in addition to the memory allocated for heap. The cache also has on-heap\n");
		sb.append("# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n");
		sb.append("# if the default 64k chunk size is used).\n");
		sb.append("# Memory is only allocated when needed.\n");
		sb.append("# file_cache_size_in_mb: 512\n");
		sb.append("\n");
		sb.append("# Flag indicating whether to allocate on or off heap when the sstable buffer\n");
		sb.append("# pool is exhausted, that is when it has exceeded the maximum memory\n");
		sb.append("# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n");
		sb.append("\n");
		sb.append("# buffer_pool_use_heap_if_exhausted: true\n");
		sb.append("\n");
		sb.append("# The strategy for optimizing disk read\n");
		sb.append("# Possible values are:\n");
		sb.append("# ssd (for solid state disks, the default)\n");
		sb.append("# spinning (for spinning disks)\n");
		sb.append("# disk_optimization_strategy: ssd\n");
		sb.append("\n");
		sb.append("# Total permitted memory to use for memtables. Cassandra will stop\n");
		sb.append("# accepting writes when the limit is exceeded until a flush completes,\n");
		sb.append("# and will trigger a flush based on memtable_cleanup_threshold\n");
		sb.append("# If omitted, Cassandra will set both to 1/4 the size of the heap.\n");
		sb.append("# memtable_heap_space_in_mb: 2048\n");
		sb.append("# memtable_offheap_space_in_mb: 2048\n");
		sb.append("\n");
		sb.append("# memtable_cleanup_threshold is deprecated. The default calculation\n");
		sb.append("# is the only reasonable choice. See the comments on  memtable_flush_writers\n");
		sb.append("# for more information.\n");
		sb.append("#\n");
		sb.append("# Ratio of occupied non-flushing memtable size to total permitted size\n");
		sb.append("# that will trigger a flush of the largest memtable. Larger mct will\n");
		sb.append("# mean larger flushes and hence less compaction, but also less concurrent\n");
		sb.append("# flush activity which can make it difficult to keep your disks fed\n");
		sb.append("# under heavy write load.\n");
		sb.append("#\n");
		sb.append("# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n");
		sb.append("# memtable_cleanup_threshold: 0.11\n");
		sb.append("\n");
		sb.append("# Specify the way Cassandra allocates and manages memtable memory.\n");
		sb.append("# Options are:\n");
		sb.append("#\n");
		sb.append("# heap_buffers\n");
		sb.append("#   on heap nio buffers\n");
		sb.append("#\n");
		sb.append("# offheap_buffers\n");
		sb.append("#   off heap (direct) nio buffers\n");
		sb.append("#\n");
		sb.append("# offheap_objects\n");
		sb.append("#    off heap objects\n");
		sb.append("memtable_allocation_type: heap_buffers\n");
		sb.append("\n");
		sb.append("# Limits the maximum Merkle tree depth to avoid consuming too much\n");
		sb.append("# memory during repairs.\n");
		sb.append("#\n");
		sb.append("# The default setting of 18 generates trees of maximum size around\n");
		sb.append("# 50 MiB / tree. If you are running out of memory during repairs consider\n");
		sb.append("# lowering this to 15 (~6 MiB / tree) or lower, but try not to lower it\n");
		sb.append("# too much past that or you will lose too much resolution and stream\n");
		sb.append("# too much redundant data during repair. Cannot be set lower than 10.\n");
		sb.append("#\n");
		sb.append("# For more details see https://issues.apache.org/jira/browse/CASSANDRA-14096.\n");
		sb.append("#\n");
		sb.append("# repair_session_max_tree_depth: 18\n");
		sb.append("\n");
		sb.append("# Total space to use for commit logs on disk.\n");
		sb.append("#\n");
		sb.append("# If space gets above this value, Cassandra will flush every dirty CF\n");
		sb.append("# in the oldest segment and remove it.  So a small total commitlog space\n");
		sb.append("# will tend to cause more flush activity on less-active columnfamilies.\n");
		sb.append("#\n");
		sb.append("# The default value is the smaller of 8192, and 1/4 of the total space\n");
		sb.append("# of the commitlog volume.\n");
		sb.append("#\n");
		sb.append("# commitlog_total_space_in_mb: 8192\n");
		sb.append("\n");
		sb.append("# This sets the number of memtable flush writer threads per disk\n");
		sb.append("# as well as the total number of memtables that can be flushed concurrently.\n");
		sb.append("# These are generally a combination of compute and IO bound.\n");
		sb.append("#\n");
		sb.append("# Memtable flushing is more CPU efficient than memtable ingest and a single thread\n");
		sb.append("# can keep up with the ingest rate of a whole server on a single fast disk\n");
		sb.append("# until it temporarily becomes IO bound under contention typically with compaction.\n");
		sb.append("# At that point you need multiple flush threads. At some point in the future\n");
		sb.append("# it may become CPU bound all the time.\n");
		sb.append("#\n");
		sb.append("# You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\n");
		sb.append("# metric which should be 0, but will be non-zero if threads are blocked waiting on flushing\n");
		sb.append("# to free memory.\n");
		sb.append("#\n");
		sb.append("# memtable_flush_writers defaults to two for a single data directory.\n");
		sb.append("# This means that two  memtables can be flushed concurrently to the single data directory.\n");
		sb.append("# If you have multiple data directories the default is one memtable flushing at a time\n");
		sb.append("# but the flush will use a thread per data directory so you will get two or more writers.\n");
		sb.append("#\n");
		sb.append("# Two is generally enough to flush on a fast disk [array] mounted as a single data directory.\n");
		sb.append("# Adding more flush writers will result in smaller more frequent flushes that introduce more\n");
		sb.append("# compaction overhead.\n");
		sb.append("#\n");
		sb.append("# There is a direct tradeoff between number of memtables that can be flushed concurrently\n");
		sb.append("# and flush size and frequency. More is not better you just need enough flush writers\n");
		sb.append("# to never stall waiting for flushing to free memory.\n");
		sb.append("#\n");
		sb.append("#memtable_flush_writers: 2\n");
		sb.append("\n");
		sb.append("# Total space to use for change-data-capture logs on disk.\n");
		sb.append("#\n");
		sb.append("# If space gets above this value, Cassandra will throw WriteTimeoutException\n");
		sb.append("# on Mutations including tables with CDC enabled. A CDCCompactor is responsible\n");
		sb.append("# for parsing the raw CDC logs and deleting them when parsing is completed.\n");
		sb.append("#\n");
		sb.append("# The default value is the min of 4096 mb and 1/8th of the total space\n");
		sb.append("# of the drive where cdc_raw_directory resides.\n");
		sb.append("# cdc_total_space_in_mb: 4096\n");
		sb.append("\n");
		sb.append("# When we hit our cdc_raw limit and the CDCCompactor is either running behind\n");
		sb.append("# or experiencing backpressure, we check at the following interval to see if any\n");
		sb.append("# new space for cdc-tracked tables has been made available. Default to 250ms\n");
		sb.append("# cdc_free_space_check_interval_ms: 250\n");
		sb.append("\n");
		sb.append("# A fixed memory pool size in MB for for SSTable index summaries. If left\n");
		sb.append("# empty, this will default to 5% of the heap size. If the memory usage of\n");
		sb.append("# all index summaries exceeds this limit, SSTables with low read rates will\n");
		sb.append("# shrink their index summaries in order to meet this limit.  However, this\n");
		sb.append("# is a best-effort process. In extreme conditions Cassandra may need to use\n");
		sb.append("# more than this amount of memory.\n");
		sb.append("index_summary_capacity_in_mb:\n");
		sb.append("\n");
		sb.append("# How frequently index summaries should be resampled.  This is done\n");
		sb.append("# periodically to redistribute memory from the fixed-size pool to sstables\n");
		sb.append("# proportional their recent read rates.  Setting to -1 will disable this\n");
		sb.append("# process, leaving existing index summaries at their current sampling level.\n");
		sb.append("index_summary_resize_interval_in_minutes: 60\n");
		sb.append("\n");
		sb.append("# Whether to, when doing sequential writing, fsync() at intervals in\n");
		sb.append("# order to force the operating system to flush the dirty\n");
		sb.append("# buffers. Enable this to avoid sudden dirty buffer flushing from\n");
		sb.append("# impacting read latencies. Almost always a good idea on SSDs; not\n");
		sb.append("# necessarily on platters.\n");
		sb.append("trickle_fsync: false\n");
		sb.append("trickle_fsync_interval_in_kb: 10240\n");
		sb.append("\n");
		sb.append("# TCP port, for commands and data\n");
		sb.append("# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n");
		sb.append("storage_port: 7000\n");
		sb.append("\n");
		sb.append("# SSL port, for encrypted communication.  Unused unless enabled in\n");
		sb.append("# encryption_options\n");
		sb.append("# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n");
		sb.append("ssl_storage_port: 7001\n");
		sb.append("\n");
		sb.append("# Address or interface to bind to and tell other Cassandra nodes to connect to.\n");
		sb.append("# You _must_ change this if you want multiple nodes to be able to communicate!\n");
		sb.append("#\n");
		sb.append("# Set listen_address OR listen_interface, not both.\n");
		sb.append("#\n");
		sb.append("# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n");
		sb.append("# will always do the Right Thing _if_ the node is properly configured\n");
		sb.append("# (hostname, name resolution, etc), and the Right Thing is to use the\n");
		sb.append("# address associated with the hostname (it might not be).\n");
		sb.append("#\n");
		sb.append("# Setting listen_address to 0.0.0.0 is always wrong.\n");
		sb.append("#\n");
		sb.append("listen_address: localhost\n");
		sb.append("\n");
		sb.append("# Set listen_address OR listen_interface, not both. Interfaces must correspond\n");
		sb.append("# to a single address, IP aliasing is not supported.\n");
		sb.append("# listen_interface: eth0\n");
		sb.append("\n");
		sb.append("# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n");
		sb.append("# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n");
		sb.append("# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n");
		sb.append("# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\n");
		sb.append("# listen_interface_prefer_ipv6: false\n");
		sb.append("\n");
		sb.append("# Address to broadcast to other Cassandra nodes\n");
		sb.append("# Leaving this blank will set it to the same value as listen_address\n");
		sb.append("# broadcast_address: 1.2.3.4\n");
		sb.append("\n");
		sb.append("# When using multiple physical network interfaces, set this\n");
		sb.append("# to true to listen on broadcast_address in addition to\n");
		sb.append("# the listen_address, allowing nodes to communicate in both\n");
		sb.append("# interfaces.\n");
		sb.append("# Ignore this property if the network configuration automatically\n");
		sb.append("# routes  between the public and private networks such as EC2.\n");
		sb.append("# listen_on_broadcast_address: false\n");
		sb.append("\n");
		sb.append("# Internode authentication backend, implementing IInternodeAuthenticator;\n");
		sb.append("# used to allow/disallow connections from peer nodes.\n");
		sb.append("# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n");
		sb.append("\n");
		sb.append("# Whether to start the native transport server.\n");
		sb.append("# Please note that the address on which the native transport is bound is the\n");
		sb.append("# same as the rpc_address. The port however is different and specified below.\n");
		sb.append("start_native_transport: true\n");
		sb.append("# port for the CQL native transport to listen for clients on\n");
		sb.append("# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n");
		sb.append("native_transport_port: 9042\n");
		sb.append("# Enabling native transport encryption in client_encryption_options allows you to either use\n");
		sb.append("# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n");
		sb.append("# standard native_transport_port.\n");
		sb.append("# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n");
		sb.append("# for native_transport_port. Setting native_transport_port_ssl to a different value\n");
		sb.append("# from native_transport_port will use encryption for native_transport_port_ssl while\n");
		sb.append("# keeping native_transport_port unencrypted.\n");
		sb.append("# native_transport_port_ssl: 9142\n");
		sb.append("# The maximum threads for handling requests when the native transport is used.\n");
		sb.append("# This is similar to rpc_max_threads though the default differs slightly (and\n");
		sb.append("# there is no native_transport_min_threads, idle threads will always be stopped\n");
		sb.append("# after 30 seconds).\n");
		sb.append("# native_transport_max_threads: 128\n");
		sb.append("#\n");
		sb.append("# The maximum size of allowed frame. Frame (requests) larger than this will\n");
		sb.append("# be rejected as invalid. The default is 256MB. If you're changing this parameter,\n");
		sb.append("# you may want to adjust max_value_size_in_mb accordingly. This should be positive and less than 2048.\n");
		sb.append("# native_transport_max_frame_size_in_mb: 256\n");
		sb.append("\n");
		sb.append("# The maximum number of concurrent client connections.\n");
		sb.append("# The default is -1, which means unlimited.\n");
		sb.append("# native_transport_max_concurrent_connections: -1\n");
		sb.append("\n");
		sb.append("# The maximum number of concurrent client connections per source ip.\n");
		sb.append("# The default is -1, which means unlimited.\n");
		sb.append("# native_transport_max_concurrent_connections_per_ip: -1\n");
		sb.append("\n");
		sb.append("# Whether to start the thrift rpc server.\n");
		sb.append("start_rpc: false\n");
		sb.append("\n");
		sb.append("# The address or interface to bind the Thrift RPC service and native transport\n");
		sb.append("# server to.\n");
		sb.append("#\n");
		sb.append("# Set rpc_address OR rpc_interface, not both.\n");
		sb.append("#\n");
		sb.append("# Leaving rpc_address blank has the same effect as on listen_address\n");
		sb.append("# (i.e. it will be based on the configured hostname of the node).\n");
		sb.append("#\n");
		sb.append("# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n");
		sb.append("# set broadcast_rpc_address to a value other than 0.0.0.0.\n");
		sb.append("#\n");
		sb.append("# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\n");
		// ユーザーが直接接続出来るようにプライベートIPアドレスを設定する
		sb.append("rpc_address: " + instance.getPrivateIpAddress() + "\n");
		sb.append("\n");
		sb.append("# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n");
		sb.append("# to a single address, IP aliasing is not supported.\n");
		sb.append("# rpc_interface: eth1\n");
		sb.append("\n");
		sb.append("# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n");
		sb.append("# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n");
		sb.append("# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n");
		sb.append("# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\n");
		sb.append("# rpc_interface_prefer_ipv6: false\n");
		sb.append("\n");
		sb.append("# port for Thrift to listen for clients on\n");
		sb.append("rpc_port: 9160\n");
		sb.append("\n");
		sb.append("# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n");
		sb.append("# be set to 0.0.0.0. If left blank, this will be set to the value of\n");
		sb.append("# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n");
		sb.append("# be set.\n");
		sb.append("# broadcast_rpc_address: 1.2.3.4\n");
		sb.append("\n");
		sb.append("# enable or disable keepalive on rpc/native connections\n");
		sb.append("rpc_keepalive: true\n");
		sb.append("\n");
		sb.append("# Cassandra provides two out-of-the-box options for the RPC Server:\n");
		sb.append("#\n");
		sb.append("# sync\n");
		sb.append("#   One thread per thrift connection. For a very large number of clients, memory\n");
		sb.append("#   will be your limiting factor. On a 64 bit JVM, 180KB is the minimum stack size\n");
		sb.append("#   per thread, and that will correspond to your use of virtual memory (but physical memory\n");
		sb.append("#   may be limited depending on use of stack space).\n");
		sb.append("#\n");
		sb.append("# hsha\n");
		sb.append("#   Stands for \\\"half synchronous, half asynchronous.\\\" All thrift clients are handled\n");
		sb.append("#   asynchronously using a small number of threads that does not vary with the amount\n");
		sb.append("#   of thrift clients (and thus scales well to many clients). The rpc requests are still\n");
		sb.append("#   synchronous (one thread per active request). If hsha is selected then it is essential\n");
		sb.append("#   that rpc_max_threads is changed from the default value of unlimited.\n");
		sb.append("#\n");
		sb.append("# The default is sync because on Windows hsha is about 30% slower.  On Linux,\n");
		sb.append("# sync/hsha performance is about the same, with hsha of course using less memory.\n");
		sb.append("#\n");
		sb.append("# Alternatively,  can provide your own RPC server by providing the fully-qualified class name\n");
		sb.append("# of an o.a.c.t.TServerFactory that can create an instance of it.\n");
		sb.append("rpc_server_type: sync\n");
		sb.append("\n");
		sb.append("# Uncomment rpc_min|max_thread to set request pool size limits.\n");
		sb.append("#\n");
		sb.append("# Regardless of your choice of RPC server (see above), the number of maximum requests in the\n");
		sb.append("# RPC thread pool dictates how many concurrent requests are possible (but if you are using the sync\n");
		sb.append("# RPC server, it also dictates the number of clients that can be connected at all).\n");
		sb.append("#\n");
		sb.append("# The default is unlimited and thus provides no protection against clients overwhelming the server. You are\n");
		sb.append("# encouraged to set a maximum that makes sense for you in production, but do keep in mind that\n");
		sb.append("# rpc_max_threads represents the maximum number of client requests this server may execute concurrently.\n");
		sb.append("#\n");
		sb.append("# rpc_min_threads: 16\n");
		sb.append("# rpc_max_threads: 2048\n");
		sb.append("\n");
		sb.append("# uncomment to set socket buffer sizes on rpc connections\n");
		sb.append("# rpc_send_buff_size_in_bytes:\n");
		sb.append("# rpc_recv_buff_size_in_bytes:\n");
		sb.append("\n");
		sb.append("# Uncomment to set socket buffer size for internode communication\n");
		sb.append("# Note that when setting this, the buffer size is limited by net.core.wmem_max\n");
		sb.append("# and when not setting it it is defined by net.ipv4.tcp_wmem\n");
		sb.append("# See also:\n");
		sb.append("# /proc/sys/net/core/wmem_max\n");
		sb.append("# /proc/sys/net/core/rmem_max\n");
		sb.append("# /proc/sys/net/ipv4/tcp_wmem\n");
		sb.append("# /proc/sys/net/ipv4/tcp_wmem\n");
		sb.append("# and 'man tcp'\n");
		sb.append("# internode_send_buff_size_in_bytes:\n");
		sb.append("\n");
		sb.append("# Uncomment to set socket buffer size for internode communication\n");
		sb.append("# Note that when setting this, the buffer size is limited by net.core.wmem_max\n");
		sb.append("# and when not setting it it is defined by net.ipv4.tcp_wmem\n");
		sb.append("# internode_recv_buff_size_in_bytes:\n");
		sb.append("\n");
		sb.append("# Frame size for thrift (maximum message length).\n");
		sb.append("thrift_framed_transport_size_in_mb: 15\n");
		sb.append("\n");
		sb.append("# Set to true to have Cassandra create a hard link to each sstable\n");
		sb.append("# flushed or streamed locally in a backups/ subdirectory of the\n");
		sb.append("# keyspace data.  Removing these links is the operator's\n");
		sb.append("# responsibility.\n");
		sb.append("incremental_backups: false\n");
		sb.append("\n");
		sb.append("# Whether or not to take a snapshot before each compaction.  Be\n");
		sb.append("# careful using this option, since Cassandra won't clean up the\n");
		sb.append("# snapshots for you.  Mostly useful if you're paranoid when there\n");
		sb.append("# is a data format change.\n");
		sb.append("snapshot_before_compaction: false\n");
		sb.append("\n");
		sb.append("# Whether or not a snapshot is taken of the data before keyspace truncation\n");
		sb.append("# or dropping of column families. The STRONGLY advised default of true \n");
		sb.append("# should be used to provide data safety. If you set this flag to false, you will\n");
		sb.append("# lose data on truncation or drop.\n");
		sb.append("auto_snapshot: true\n");
		sb.append("\n");
		sb.append("# Granularity of the collation index of rows within a partition.\n");
		sb.append("# Increase if your rows are large, or if you have a very large\n");
		sb.append("# number of rows per partition.  The competing goals are these:\n");
		sb.append("#\n");
		sb.append("# - a smaller granularity means more index entries are generated\n");
		sb.append("#   and looking up rows withing the partition by collation column\n");
		sb.append("#   is faster\n");
		sb.append("# - but, Cassandra will keep the collation index in memory for hot\n");
		sb.append("#   rows (as part of the key cache), so a larger granularity means\n");
		sb.append("#   you can cache more hot rows\n");
		sb.append("column_index_size_in_kb: 64\n");
		sb.append("\n");
		sb.append("# Per sstable indexed key cache entries (the collation index in memory\n");
		sb.append("# mentioned above) exceeding this size will not be held on heap.\n");
		sb.append("# This means that only partition information is held on heap and the\n");
		sb.append("# index entries are read from disk.\n");
		sb.append("#\n");
		sb.append("# Note that this size refers to the size of the\n");
		sb.append("# serialized index information and not the size of the partition.\n");
		sb.append("column_index_cache_size_in_kb: 2\n");
		sb.append("\n");
		sb.append("# Number of simultaneous compactions to allow, NOT including\n");
		sb.append("# validation \\\"compactions\\\" for anti-entropy repair.  Simultaneous\n");
		sb.append("# compactions can help preserve read performance in a mixed read/write\n");
		sb.append("# workload, by mitigating the tendency of small sstables to accumulate\n");
		sb.append("# during a single long running compactions. The default is usually\n");
		sb.append("# fine and if you experience problems with compaction running too\n");
		sb.append("# slowly or too fast, you should look at\n");
		sb.append("# compaction_throughput_mb_per_sec first.\n");
		sb.append("#\n");
		sb.append("# concurrent_compactors defaults to the smaller of (number of disks,\n");
		sb.append("# number of cores), with a minimum of 2 and a maximum of 8.\n");
		sb.append("# \n");
		sb.append("# If your data directories are backed by SSD, you should increase this\n");
		sb.append("# to the number of cores.\n");
		sb.append("#concurrent_compactors: 1\n");
		sb.append("\n");
		sb.append("# Throttles compaction to the given total throughput across the entire\n");
		sb.append("# system. The faster you insert data, the faster you need to compact in\n");
		sb.append("# order to keep the sstable count down, but in general, setting this to\n");
		sb.append("# 16 to 32 times the rate you are inserting data is more than sufficient.\n");
		sb.append("# Setting this to 0 disables throttling. Note that this account for all types\n");
		sb.append("# of compaction, including validation compaction.\n");
		sb.append("compaction_throughput_mb_per_sec: 16\n");
		sb.append("\n");
		sb.append("# When compacting, the replacement sstable(s) can be opened before they\n");
		sb.append("# are completely written, and used in place of the prior sstables for\n");
		sb.append("# any range that has been written. This helps to smoothly transfer reads \n");
		sb.append("# between the sstables, reducing page cache churn and keeping hot rows hot\n");
		sb.append("sstable_preemptive_open_interval_in_mb: 50\n");
		sb.append("\n");
		sb.append("# Throttles all outbound streaming file transfers on this node to the\n");
		sb.append("# given total throughput in Mbps. This is necessary because Cassandra does\n");
		sb.append("# mostly sequential IO when streaming data during bootstrap or repair, which\n");
		sb.append("# can lead to saturating the network connection and degrading rpc performance.\n");
		sb.append("# When unset, the default is 200 Mbps or 25 MB/s.\n");
		sb.append("# stream_throughput_outbound_megabits_per_sec: 200\n");
		sb.append("\n");
		sb.append("# Throttles all streaming file transfer between the datacenters,\n");
		sb.append("# this setting allows users to throttle inter dc stream throughput in addition\n");
		sb.append("# to throttling all network stream traffic as configured with\n");
		sb.append("# stream_throughput_outbound_megabits_per_sec\n");
		sb.append("# When unset, the default is 200 Mbps or 25 MB/s\n");
		sb.append("# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n");
		sb.append("\n");
		sb.append("# How long the coordinator should wait for read operations to complete\n");
		sb.append("read_request_timeout_in_ms: 5000\n");
		sb.append("# How long the coordinator should wait for seq or index scans to complete\n");
		sb.append("range_request_timeout_in_ms: 10000\n");
		sb.append("# How long the coordinator should wait for writes to complete\n");
		sb.append("write_request_timeout_in_ms: 2000\n");
		sb.append("# How long the coordinator should wait for counter writes to complete\n");
		sb.append("counter_write_request_timeout_in_ms: 5000\n");
		sb.append("# How long a coordinator should continue to retry a CAS operation\n");
		sb.append("# that contends with other proposals for the same row\n");
		sb.append("cas_contention_timeout_in_ms: 1000\n");
		sb.append("# How long the coordinator should wait for truncates to complete\n");
		sb.append("# (This can be much longer, because unless auto_snapshot is disabled\n");
		sb.append("# we need to flush first so we can snapshot before removing the data.)\n");
		sb.append("truncate_request_timeout_in_ms: 60000\n");
		sb.append("# The default timeout for other, miscellaneous operations\n");
		sb.append("request_timeout_in_ms: 10000\n");
		sb.append("\n");
		sb.append("# How long before a node logs slow queries. Select queries that take longer than\n");
		sb.append("# this timeout to execute, will generate an aggregated log message, so that slow queries\n");
		sb.append("# can be identified. Set this value to zero to disable slow query logging.\n");
		sb.append("slow_query_log_timeout_in_ms: 500\n");
		sb.append("\n");
		sb.append("# Enable operation timeout information exchange between nodes to accurately\n");
		sb.append("# measure request timeouts.  If disabled, replicas will assume that requests\n");
		sb.append("# were forwarded to them instantly by the coordinator, which means that\n");
		sb.append("# under overload conditions we will waste that much extra time processing \n");
		sb.append("# already-timed-out requests.\n");
		sb.append("#\n");
		sb.append("# Warning: before enabling this property make sure to ntp is installed\n");
		sb.append("# and the times are synchronized between the nodes.\n");
		sb.append("cross_node_timeout: false\n");
		sb.append("\n");
		sb.append("# Set keep-alive period for streaming\n");
		sb.append("# This node will send a keep-alive message periodically with this period.\n");
		sb.append("# If the node does not receive a keep-alive message from the peer for\n");
		sb.append("# 2 keep-alive cycles the stream session times out and fail\n");
		sb.append("# Default value is 300s (5 minutes), which means stalled stream\n");
		sb.append("# times out in 10 minutes by default\n");
		sb.append("# streaming_keep_alive_period_in_secs: 300\n");
		sb.append("\n");
		sb.append("# phi value that must be reached for a host to be marked down.\n");
		sb.append("# most users should never need to adjust this.\n");
		sb.append("# phi_convict_threshold: 8\n");
		sb.append("\n");
		sb.append("# endpoint_snitch -- Set this to a class that implements\n");
		sb.append("# IEndpointSnitch.  The snitch has two functions:\n");
		sb.append("#\n");
		sb.append("# - it teaches Cassandra enough about your network topology to route\n");
		sb.append("#   requests efficiently\n");
		sb.append("# - it allows Cassandra to spread replicas around your cluster to avoid\n");
		sb.append("#   correlated failures. It does this by grouping machines into\n");
		sb.append("#   \\\"datacenters\\\" and \\\"racks.\\\"  Cassandra will do its best not to have\n");
		sb.append("#   more than one replica on the same \\\"rack\\\" (which may not actually\n");
		sb.append("#   be a physical location)\n");
		sb.append("#\n");
		sb.append("# CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\n");
		sb.append("# ONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.\n");
		sb.append("# This means that if you start with the default SimpleSnitch, which\n");
		sb.append("# locates every node on \\\"rack1\\\" in \\\"datacenter1\\\", your only options\n");
		sb.append("# if you need to add another datacenter are GossipingPropertyFileSnitch\n");
		sb.append("# (and the older PFS).  From there, if you want to migrate to an\n");
		sb.append("# incompatible snitch like Ec2Snitch you can do it by adding new nodes\n");
		sb.append("# under Ec2Snitch (which will locate them in a new \\\"datacenter\\\") and\n");
		sb.append("# decommissioning the old ones.\n");
		sb.append("#\n");
		sb.append("# Out of the box, Cassandra provides:\n");
		sb.append("#\n");
		sb.append("# SimpleSnitch:\n");
		sb.append("#    Treats Strategy order as proximity. This can improve cache\n");
		sb.append("#    locality when disabling read repair.  Only appropriate for\n");
		sb.append("#    single-datacenter deployments.\n");
		sb.append("#\n");
		sb.append("# GossipingPropertyFileSnitch\n");
		sb.append("#    This should be your go-to snitch for production use.  The rack\n");
		sb.append("#    and datacenter for the local node are defined in\n");
		sb.append("#    cassandra-rackdc.properties and propagated to other nodes via\n");
		sb.append("#    gossip.  If cassandra-topology.properties exists, it is used as a\n");
		sb.append("#    fallback, allowing migration from the PropertyFileSnitch.\n");
		sb.append("#\n");
		sb.append("# PropertyFileSnitch:\n");
		sb.append("#    Proximity is determined by rack and data center, which are\n");
		sb.append("#    explicitly configured in cassandra-topology.properties.\n");
		sb.append("#\n");
		sb.append("# Ec2Snitch:\n");
		sb.append("#    Appropriate for EC2 deployments in a single Region. Loads Region\n");
		sb.append("#    and Availability Zone information from the EC2 API. The Region is\n");
		sb.append("#    treated as the datacenter, and the Availability Zone as the rack.\n");
		sb.append("#    Only private IPs are used, so this will not work across multiple\n");
		sb.append("#    Regions.\n");
		sb.append("#\n");
		sb.append("# Ec2MultiRegionSnitch:\n");
		sb.append("#    Uses public IPs as broadcast_address to allow cross-region\n");
		sb.append("#    connectivity.  (Thus, you should set seed addresses to the public\n");
		sb.append("#    IP as well.) You will need to open the storage_port or\n");
		sb.append("#    ssl_storage_port on the public IP firewall.  (For intra-Region\n");
		sb.append("#    traffic, Cassandra will switch to the private IP after\n");
		sb.append("#    establishing a connection.)\n");
		sb.append("#\n");
		sb.append("# RackInferringSnitch:\n");
		sb.append("#    Proximity is determined by rack and data center, which are\n");
		sb.append("#    assumed to correspond to the 3rd and 2nd octet of each node's IP\n");
		sb.append("#    address, respectively.  Unless this happens to match your\n");
		sb.append("#    deployment conventions, this is best used as an example of\n");
		sb.append("#    writing a custom Snitch class and is provided in that spirit.\n");
		sb.append("#\n");
		sb.append("# You can use a custom Snitch by setting this to the full class name\n");
		sb.append("# of the snitch, which will be assumed to be on your classpath.\n");
		sb.append("endpoint_snitch: SimpleSnitch\n");
		sb.append("\n");
		sb.append("# controls how often to perform the more expensive part of host score\n");
		sb.append("# calculation\n");
		sb.append("dynamic_snitch_update_interval_in_ms: 100 \n");
		sb.append("# controls how often to reset all host scores, allowing a bad host to\n");
		sb.append("# possibly recover\n");
		sb.append("dynamic_snitch_reset_interval_in_ms: 600000\n");
		sb.append("# if set greater than zero and read_repair_chance is < 1.0, this will allow\n");
		sb.append("# 'pinning' of replicas to hosts in order to increase cache capacity.\n");
		sb.append("# The badness threshold will control how much worse the pinned host has to be\n");
		sb.append("# before the dynamic snitch will prefer other replicas over it.  This is\n");
		sb.append("# expressed as a double which represents a percentage.  Thus, a value of\n");
		sb.append("# 0.2 means Cassandra would continue to prefer the static snitch values\n");
		sb.append("# until the pinned host was 20% worse than the fastest.\n");
		sb.append("dynamic_snitch_badness_threshold: 0.1\n");
		sb.append("\n");
		sb.append("# request_scheduler -- Set this to a class that implements\n");
		sb.append("# RequestScheduler, which will schedule incoming client requests\n");
		sb.append("# according to the specific policy. This is useful for multi-tenancy\n");
		sb.append("# with a single Cassandra cluster.\n");
		sb.append("# NOTE: This is specifically for requests from the client and does\n");
		sb.append("# not affect inter node communication.\n");
		sb.append("# org.apache.cassandra.scheduler.NoScheduler - No scheduling takes place\n");
		sb.append("# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin of\n");
		sb.append("# client requests to a node with a separate queue for each\n");
		sb.append("# request_scheduler_id. The scheduler is further customized by\n");
		sb.append("# request_scheduler_options as described below.\n");
		sb.append("request_scheduler: org.apache.cassandra.scheduler.NoScheduler\n");
		sb.append("\n");
		sb.append("# Scheduler Options vary based on the type of scheduler\n");
		sb.append("#\n");
		sb.append("# NoScheduler\n");
		sb.append("#   Has no options\n");
		sb.append("#\n");
		sb.append("# RoundRobin\n");
		sb.append("#   throttle_limit\n");
		sb.append("#     The throttle_limit is the number of in-flight\n");
		sb.append("#     requests per client.  Requests beyond \n");
		sb.append("#     that limit are queued up until\n");
		sb.append("#     running requests can complete.\n");
		sb.append("#     The value of 80 here is twice the number of\n");
		sb.append("#     concurrent_reads + concurrent_writes.\n");
		sb.append("#   default_weight\n");
		sb.append("#     default_weight is optional and allows for\n");
		sb.append("#     overriding the default which is 1.\n");
		sb.append("#   weights\n");
		sb.append("#     Weights are optional and will default to 1 or the\n");
		sb.append("#     overridden default_weight. The weight translates into how\n");
		sb.append("#     many requests are handled during each turn of the\n");
		sb.append("#     RoundRobin, based on the scheduler id.\n");
		sb.append("#\n");
		sb.append("# request_scheduler_options:\n");
		sb.append("#    throttle_limit: 80\n");
		sb.append("#    default_weight: 5\n");
		sb.append("#    weights:\n");
		sb.append("#      Keyspace1: 1\n");
		sb.append("#      Keyspace2: 5\n");
		sb.append("\n");
		sb.append("# request_scheduler_id -- An identifier based on which to perform\n");
		sb.append("# the request scheduling. Currently the only valid option is keyspace.\n");
		sb.append("# request_scheduler_id: keyspace\n");
		sb.append("\n");
		sb.append("# Enable or disable inter-node encryption\n");
		sb.append("# JVM defaults for supported SSL socket protocols and cipher suites can\n");
		sb.append("# be replaced using custom encryption options. This is not recommended\n");
		sb.append("# unless you have policies in place that dictate certain settings, or\n");
		sb.append("# need to disable vulnerable ciphers or protocols in case the JVM cannot\n");
		sb.append("# be updated.\n");
		sb.append("# FIPS compliant settings can be configured at JVM level and should not\n");
		sb.append("# involve changing encryption settings here:\n");
		sb.append("# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n");
		sb.append("# *NOTE* No custom encryption options are enabled at the moment\n");
		sb.append("# The available internode options are : all, none, dc, rack\n");
		sb.append("#\n");
		sb.append("# If set to dc cassandra will encrypt the traffic between the DCs\n");
		sb.append("# If set to rack cassandra will encrypt the traffic between the racks\n");
		sb.append("#\n");
		sb.append("# The passwords used in these options must match the passwords used when generating\n");
		sb.append("# the keystore and truststore.  For instructions on generating these files, see:\n");
		sb.append("# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n");
		sb.append("#\n");
		sb.append("server_encryption_options:\n");
		sb.append("    internode_encryption: none\n");
		sb.append("    keystore: conf/.keystore\n");
		sb.append("    keystore_password: cassandra\n");
		sb.append("    truststore: conf/.truststore\n");
		sb.append("    truststore_password: cassandra\n");
		sb.append("    # More advanced defaults below:\n");
		sb.append("    # protocol: TLS\n");
		sb.append("    # algorithm: SunX509\n");
		sb.append("    # store_type: JKS\n");
		sb.append("    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n");
		sb.append("    # require_client_auth: false\n");
		sb.append("    # require_endpoint_verification: false\n");
		sb.append("\n");
		sb.append("# enable or disable client/server encryption.\n");
		sb.append("client_encryption_options:\n");
		sb.append("    enabled: false\n");
		sb.append("    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n");
		sb.append("    optional: false\n");
		sb.append("    keystore: conf/.keystore\n");
		sb.append("    keystore_password: cassandra\n");
		sb.append("    # require_client_auth: false\n");
		sb.append("    # Set trustore and truststore_password if require_client_auth is true\n");
		sb.append("    # truststore: conf/.truststore\n");
		sb.append("    # truststore_password: cassandra\n");
		sb.append("    # More advanced defaults below:\n");
		sb.append("    # protocol: TLS\n");
		sb.append("    # algorithm: SunX509\n");
		sb.append("    # store_type: JKS\n");
		sb.append("    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n");
		sb.append("\n");
		sb.append("# internode_compression controls whether traffic between nodes is\n");
		sb.append("# compressed.\n");
		sb.append("# Can be:\n");
		sb.append("#\n");
		sb.append("# all\n");
		sb.append("#   all traffic is compressed\n");
		sb.append("#\n");
		sb.append("# dc\n");
		sb.append("#   traffic between different datacenters is compressed\n");
		sb.append("#\n");
		sb.append("# none\n");
		sb.append("#   nothing is compressed.\n");
		sb.append("internode_compression: dc\n");
		sb.append("\n");
		sb.append("# Enable or disable tcp_nodelay for inter-dc communication.\n");
		sb.append("# Disabling it will result in larger (but fewer) network packets being sent,\n");
		sb.append("# reducing overhead from the TCP protocol itself, at the cost of increasing\n");
		sb.append("# latency if you block for cross-datacenter responses.\n");
		sb.append("inter_dc_tcp_nodelay: false\n");
		sb.append("\n");
		sb.append("# TTL for different trace types used during logging of the repair process.\n");
		sb.append("tracetype_query_ttl: 86400\n");
		sb.append("tracetype_repair_ttl: 604800\n");
		sb.append("\n");
		sb.append("# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\n");
		sb.append("# This threshold can be adjusted to minimize logging if necessary\n");
		sb.append("# gc_log_threshold_in_ms: 200\n");
		sb.append("\n");
		sb.append("# If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at\n");
		sb.append("# INFO level\n");
		sb.append("# UDFs (user defined functions) are disabled by default.\n");
		sb.append("# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\n");
		sb.append("enable_user_defined_functions: false\n");
		sb.append("\n");
		sb.append("# Enables scripted UDFs (JavaScript UDFs).\n");
		sb.append("# Java UDFs are always enabled, if enable_user_defined_functions is true.\n");
		sb.append("# Enable this option to be able to use UDFs with \\\"language javascript\\\" or any custom JSR-223 provider.\n");
		sb.append("# This option has no effect, if enable_user_defined_functions is false.\n");
		sb.append("enable_scripted_user_defined_functions: false\n");
		sb.append("\n");
		sb.append("# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n");
		sb.append("# Lowering this value on Windows can provide much tighter latency and better throughput, however\n");
		sb.append("# some virtualized environments may see a negative performance impact from changing this setting\n");
		sb.append("# below their system default. The sysinternals 'clockres' tool can confirm your system's default\n");
		sb.append("# setting.\n");
		sb.append("windows_timer_interval: 1\n");
		sb.append("\n");
		sb.append("\n");
		sb.append("# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n");
		sb.append("# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n");
		sb.append("# the \\\"key_alias\\\" is the only key that will be used for encrypt opertaions; previously used keys\n");
		sb.append("# can still (and should!) be in the keystore and will be used on decrypt operations\n");
		sb.append("# (to handle the case of key rotation).\n");
		sb.append("#\n");
		sb.append("# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n");
		sb.append("# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n");
		sb.append("# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n");
		sb.append("#\n");
		sb.append("# Currently, only the following file types are supported for transparent data encryption, although\n");
		sb.append("# more are coming in future cassandra releases: commitlog, hints\n");
		sb.append("transparent_data_encryption_options:\n");
		sb.append("    enabled: false\n");
		sb.append("    chunk_length_kb: 64\n");
		sb.append("    cipher: AES/CBC/PKCS5Padding\n");
		sb.append("    key_alias: testing:1\n");
		sb.append("    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n");
		sb.append("    # iv_length: 16\n");
		sb.append("    key_provider: \n");
		sb.append("      - class_name: org.apache.cassandra.security.JKSKeyProvider\n");
		sb.append("        parameters: \n");
		sb.append("          - keystore: conf/.keystore\n");
		sb.append("            keystore_password: cassandra\n");
		sb.append("            store_type: JCEKS\n");
		sb.append("            key_password: cassandra\n");
		sb.append("\n");
		sb.append("\n");
		sb.append("#####################\n");
		sb.append("# SAFETY THRESHOLDS #\n");
		sb.append("#####################\n");
		sb.append("\n");
		sb.append("# When executing a scan, within or across a partition, we need to keep the\n");
		sb.append("# tombstones seen in memory so we can return them to the coordinator, which\n");
		sb.append("# will use them to make sure other replicas also know about the deleted rows.\n");
		sb.append("# With workloads that generate a lot of tombstones, this can cause performance\n");
		sb.append("# problems and even exaust the server heap.\n");
		sb.append("# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n");
		sb.append("# Adjust the thresholds here if you understand the dangers and want to\n");
		sb.append("# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n");
		sb.append("# using the StorageService mbean.\n");
		sb.append("tombstone_warn_threshold: 1000\n");
		sb.append("tombstone_failure_threshold: 100000\n");
		sb.append("\n");
		sb.append("# Log WARN on any multiple-partition batch size exceeding this value. 5kb per batch by default.\n");
		sb.append("# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\n");
		sb.append("batch_size_warn_threshold_in_kb: 5\n");
		sb.append("\n");
		sb.append("# Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.\n");
		sb.append("batch_size_fail_threshold_in_kb: 50\n");
		sb.append("\n");
		sb.append("# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\n");
		sb.append("unlogged_batch_across_partitions_warn_threshold: 10\n");
		sb.append("\n");
		sb.append("# Log a warning when compacting partitions larger than this value\n");
		sb.append("compaction_large_partition_warning_threshold_mb: 100\n");
		sb.append("\n");
		sb.append("# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n");
		sb.append("# Adjust the threshold based on your application throughput requirement\n");
		sb.append("# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\n");
		sb.append("gc_warn_threshold_in_ms: 1000\n");
		sb.append("\n");
		sb.append("# Maximum size of any value in SSTables. Safety measure to detect SSTable corruption\n");
		sb.append("# early. Any value size larger than this threshold will result into marking an SSTable\n");
		sb.append("# as corrupted. This should be positive and less than 2048.\n");
		sb.append("# max_value_size_in_mb: 256\n");
		sb.append("\n");
		sb.append("# Back-pressure settings #\n");
		sb.append("# If enabled, the coordinator will apply the back-pressure strategy specified below to each mutation\n");
		sb.append("# sent to replicas, with the aim of reducing pressure on overloaded replicas.\n");
		sb.append("back_pressure_enabled: false\n");
		sb.append("# The back-pressure strategy applied.\n");
		sb.append("# The default implementation, RateBasedBackPressure, takes three arguments:\n");
		sb.append("# high ratio, factor, and flow type, and uses the ratio between incoming mutation responses and outgoing mutation requests.\n");
		sb.append("# If below high ratio, outgoing mutations are rate limited according to the incoming rate decreased by the given factor;\n");
		sb.append("# if above high ratio, the rate limiting is increased by the given factor;\n");
		sb.append("# such factor is usually best configured between 1 and 10, use larger values for a faster recovery\n");
		sb.append("# at the expense of potentially more dropped mutations;\n");
		sb.append("# the rate limiting is applied according to the flow type: if FAST, it's rate limited at the speed of the fastest replica,\n");
		sb.append("# if SLOW at the speed of the slowest one.\n");
		sb.append("# New strategies can be added. Implementors need to implement org.apache.cassandra.net.BackpressureStrategy and\n");
		sb.append("# provide a public constructor accepting a Map<String, Object>.\n");
		sb.append("back_pressure_strategy:\n");
		sb.append("    - class_name: org.apache.cassandra.net.RateBasedBackPressure\n");
		sb.append("      parameters:\n");
		sb.append("        - high_ratio: 0.90\n");
		sb.append("          factor: 5\n");
		sb.append("          flow: FAST\n");
		sb.append("\n");
		sb.append("# Coalescing Strategies #\n");
		sb.append("# Coalescing multiples messages turns out to significantly boost message processing throughput (think doubling or more).\n");
		sb.append("# On bare metal, the floor for packet processing throughput is high enough that many applications won't notice, but in\n");
		sb.append("# virtualized environments, the point at which an application can be bound by network packet processing can be\n");
		sb.append("# surprisingly low compared to the throughput of task processing that is possible inside a VM. It's not that bare metal\n");
		sb.append("# doesn't benefit from coalescing messages, it's that the number of packets a bare metal network interface can process\n");
		sb.append("# is sufficient for many applications such that no load starvation is experienced even without coalescing.\n");
		sb.append("# There are other benefits to coalescing network messages that are harder to isolate with a simple metric like messages\n");
		sb.append("# per second. By coalescing multiple tasks together, a network thread can process multiple messages for the cost of one\n");
		sb.append("# trip to read from a socket, and all the task submission work can be done at the same time reducing context switching\n");
		sb.append("# and increasing cache friendliness of network message processing.\n");
		sb.append("# See CASSANDRA-8692 for details.\n");
		sb.append("\n");
		sb.append("# Strategy to use for coalescing messages in OutboundTcpConnection.\n");
		sb.append("# Can be fixed, movingaverage, timehorizon, disabled (default).\n");
		sb.append("# You can also specify a subclass of CoalescingStrategies.CoalescingStrategy by name.\n");
		sb.append("# otc_coalescing_strategy: DISABLED\n");
		sb.append("\n");
		sb.append("# How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first\n");
		sb.append("# message is received before it will be sent with any accompanying messages. For moving average this is the\n");
		sb.append("# maximum amount of time that will be waited as well as the interval at which messages must arrive on average\n");
		sb.append("# for coalescing to be enabled.\n");
		sb.append("# otc_coalescing_window_us: 200\n");
		sb.append("\n");
		sb.append("# Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.\n");
		sb.append("# otc_coalescing_enough_coalesced_messages: 8\n");
		sb.append("\n");
		sb.append("# How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.\n");
		sb.append("# Expiration is done if messages are piling up in the backlog. Droppable messages are expired to free the memory\n");
		sb.append("# taken by expired messages. The interval should be between 0 and 1000, and in most installations the default value\n");
		sb.append("# will be appropriate. A smaller value could potentially expire messages slightly sooner at the expense of more CPU\n");
		sb.append("# time and queue contention while iterating the backlog of messages.\n");
		sb.append("# An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.\n");
		sb.append("#\n");
		sb.append("# otc_backlog_expiration_interval_ms: 200\n");
		sb.append("\n");
		sb.append("\n");
		sb.append("#########################\n");
		sb.append("# EXPERIMENTAL FEATURES #\n");
		sb.append("#########################\n");
		sb.append("\n");
		sb.append("# Enables materialized view creation on this node.\n");
		sb.append("# Materialized views are considered experimental and are not recommended for production use.\n");
		sb.append("enable_materialized_views: true\n");
		sb.append("\n");
		sb.append("# Enables SASI index creation on this node.\n");
		sb.append("# SASI indexes are considered experimental and are not recommended for production use.\n");
		sb.append("enable_sasi_indexes: true\n");
		sb.append("\n");
		detail = sb.toString();
	}
}
